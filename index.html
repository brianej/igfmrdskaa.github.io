<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Integrating Generic Foundation Models and Reasoning with Domain-Specific Knowledge for Assistive Agents.">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Integrating LLM, Non-monotonic Logical Reasoning, and Human-in-the-loop Feedback for an Embodied AI Agent</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/all.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Integrating LLM, Non-monotonic Logical Reasoning, and Human-in-the-loop Feedback for an Embodied AI Agent</h1>
          <div class="is-size-3 publication-authors">
            <span class="author-block">
              <a>Tianyi Fu</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a>Brian Jauw</a><sup>1</sup>,</span>
            </span>
            <span class="author-block">
              <a href="https://homepages.inf.ed.ac.uk/msridhar/">Mohan Sridharan</a><sup>1</sup></span>
            </span>
          </div>

          <div class="is-size-3 publication-authors">
            <span class="author-block"><sup>1</sup>Institute of Perception, Action, and Behavior, School of Informatics, University of Edinburgh</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Tianyi-Fu/RobotAssistant"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Code</span>
                  </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Large Language Models (LLMs) are considered state of the art for many tasks in robotics and AI, 
            but there is also increasing evidence of their critical limitations. They can generate arbitrary 
            responses in new situations, do not support rapid incremental adaptation based on limited examples, 
            and are opaque. Toward addressing these limitations, our architecture leverages the complementary strengths of LLMs 
            and knowledge-based reasoning, enabling an  AI agent assisting a human user to use an LLM to provide generic 
            abstract predictions of upcoming tasks. Also, the agent reasons with domain-specific knowledge, 
            any recent history of interactions with the user, and semantic databases to: (a) provide contextual prompts to the LLM; 
            and (b) compute a plan of concrete actions that jointly implements the current task and prepare for the anticipated task, 
            diagnosing unexpected outcomes and replanning as needed. Furthermore, the architecture solicits 
            and uses high-level human feedback based on need and availability to incrementally revise the domain-specific knowledge 
            and interactions with the LLM. We ground and evaluate our architectureâ€™s abilities in the realistic 
            <em>VirtualHome</em> simulation environment, demonstrating a substantial performance improvement compared with just using 
            LLMs or an LLM and logical reasoner. 
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    
    <div class="columns is-centered">
      <div class="column is-half">
        <figure class="image">
          <img src="./static/images/framework_1.png" alt="Framework Diagram 1">
        </figure>
      </div>
    </div>

    <!-- Second Framework Diagram -->
    <div class="columns is-centered">
      <div class="column is-three-quarters">
        <figure class="image">
          <img src="./static/images/framework_2.png" alt="Framework Diagram 2">
        </figure>
      </div>
    </div>
  

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe width="100%" height="400"
          src="https://youtu.be/o_Bn5y5DUBY"
          frameborder="0"
          allow="autoplay; encrypted-media"
          allowfullscreen></iframe>

        </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The template for this page was borrowed from <a
              href="https://nerfies.github.io/">https://nerfies.github.io/</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
